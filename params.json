{"name":"Practical-machine-learning","tagline":"This repository contains files submitted in completion of the Practical Machine Learning course, which is one of nine courses from the data science specialization offered by Johns Hopkins University's Department of Biostatistics on coursera.org","body":"Human Activities Recognition Using Random Forests\r\n========================================================\r\nauthor: Jonathan Hill\r\ndate: `r date()`\r\ntransition: rotate\r\ntransition-speed: slow\r\n\r\nOriginal Authors \r\n========================================================\r\nThis dataset is licensed under the Creative Commons license (CC BY-SA).\r\n\r\nVelloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13). Stuttgart, Germany: ACM SIGCHI, 2013.\r\n\r\nRead more: http://groupware.les.inf.puc-rio.br/har#ixzz3abHmT200\r\n\r\nGoal\r\n========================================================\r\n\r\n* Using raw data from devices such as Jawbone Up, Nike FuelBand, and Fitbit \r\n+ Predict when someone is performing bicep curls with poor form and classify what they are doing wrong (classes B, C, D, and E)\r\n\r\nIntroduction\r\n========================================================\r\nSix young health participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions:\r\n\r\nClass | Description\r\n------------- | -------------\r\nA | exactly according to the specification\r\nB | throwing the elbows to the front\r\nC | lifting the dumbbell only halfway\r\nD | lowering the dumbbell only halfway\r\nE | throwing the hips to the front\r\n\r\nSensor Locations\r\n========================================================\r\n![Sensor Image](http://groupware.les.inf.puc-rio.br/static/WLE/on-body-sensing-schema.png)\r\n\r\nDownloading the Data\r\n========================================================\r\n\r\n```{r,eval=FALSE}\r\n# The training data #\r\nurl <- \"http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv\"\r\ndownload.file(url = url, destfile = \"pml-training.csv\")\r\n\r\n# The testing data #\r\nurl2 <- \"http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv\"\r\ndownload.file(url = url2, destfile = \"pml-testing.csv\")\r\n```\r\n```{r}\r\nfinaltesting <- read.csv(\"pml-testing.csv\")\r\ntraining <- read.csv(\"pml-training.csv\")\r\n```\r\n\r\nRequired R Packages\r\n========================================================\r\n```{r}\r\nlibrary(caret)\r\nlibrary(randomForest)\r\n```\r\n\r\nPartitions for Cross Validation\r\n========================================================\r\n\r\nIn order to train the model, I created a partition with 60% of the data called \"tr.\" \r\n\r\nWith the remaining 40%, I created a partition with 20% to test the model (\"te\") and another with the remaining 20% to validate the model (\"validation\").\r\n```{r}\r\ninTrain <- createDataPartition(training$classe, p = 0.6, list = FALSE)\r\ntr <- training[inTrain, ]\r\nte <- training[-inTrain, ]\r\n```\r\n\r\nPre-processing\r\n========================================================\r\n```{r}\r\n# Remove near zero variance predictors #\r\nnzv <- nearZeroVar(tr, saveMetrics = TRUE)\r\nnearzerofilter <- rownames(nzv[nzv$nzv == FALSE, ])\r\ntr <- tr[, nearzerofilter]\r\n\r\n# ... correlated predictors using a .85 cutoff #\r\ntr <- tr[, 6:ncol(tr)]\r\ntrainCor <- cor(tr[,1:(ncol(tr)-1)], use = \"complete.obs\")\r\ncorFilter <- findCorrelation(trainCor, cutoff = .85)\r\ntr <- tr[, -corFilter]\r\n\r\n# ... and variables with greater than 85% NA\r\ntr <- tr[, colMeans(is.na(tr)) <= .85]\r\n```\r\n\r\nModel Selection\r\n========================================================\r\nBecause the decision points of the model do not need to be interpreted and the continuous variables in the data are not very interpretable, random forests is a good option for this problem.\r\n\r\nCentering and scaling continuous variables prevents large values from biasing the model.\r\n\r\nThere was very little difference in accuracy among models using 3, 5 and 10-fold cross-validated resampling, but processing time was much longer for the model using 10-fold cross-validated resampling.  Therefore, the final model uses 5-fold cross-validated resampling.\r\n\r\nModel\r\n========================================================\r\n```{r}\r\n# Random forests #\r\nmodelFit <- train(classe ~.,\r\n                  data = tr,\r\n                  method = \"rf\",\r\n                  trControl = trainControl(method = \"cv\", number = 5),\r\n                  preProcess = c(\"center\", \"scale\")\r\n                  )\r\n```\r\n\r\nModel (cont.)\r\n========================================================\r\n```\r\n`r modelFit$modelInfo$label`\r\n\r\n`r paste(dim(tr)[1], \"samples\",collapse = \" \")`\r\n`r paste(dim(tr)[2], \"predictors\",collapse = \" \")`\r\n`r paste(length(levels(modelFit$finalModel$predicted)), \"classes:\", paste(levels(modelFit$finalModel$predicted),collapse = \", \"), collapse = \" \")`\r\n\r\n`r paste(\"Pre-processing:\",paste(modelFit$preProcess$method, collapse = \", \"),collapse = \" \")`\r\n`r paste(\"Resampling: Cross-Validated (\", modelFit$preProcess$k, \"fold)\", collapse = \" \")`\r\n```\r\n```{r,echo=FALSE}\r\nas.data.frame(modelFit$results[,1:4])\r\n```\r\n\r\nTest and Validation Partitions\r\n========================================================\r\n```{r}\r\n# 20% each #\r\ninValidation <- createDataPartition(te$classe, p = .5, list = FALSE)\r\nvalidation <- te[inValidation, names(te) %in% names(tr) ]\r\nte <- te[-inValidation, names(te) %in% names(tr)]\r\n```\r\nThese two partitions will help estimate the out-of-sample error of the model because they represent 40% of the data. They were not used during the model design and data cleaning steps, and they will help show how well the model could perform in the real world.\r\n\r\nTest Confusion Matrix\r\n========================================================\r\n```{r,eval=FALSE}\r\npred <- predict(modelFit, newdata = te)\r\n```\r\n```{r, echo=FALSE}\r\n# Predictions and confusion matrix for test sample #\r\npred <- predict(modelFit, newdata = te)\r\nconfusionMatrix(te$classe, pred)[[2]]\r\n```\r\nThe mean out-of-sample accuracy in the test sample is 0.9954 with a 95% confidence interval of 0.9928 to 0.9973.\r\n\r\nValidation Confusion Matrix\r\n========================================================\r\n```{r,eval=FALSE}\r\npred2 <- predict(modelFit, newdata = validation)\r\n```\r\n\r\n```{r, echo=FALSE}\r\n# Predictions and confusion matrix for validation #\r\npred2 <- predict(modelFit, newdata = validation)\r\nconfusionMatrix(validation$classe, pred2)[[2]]\r\n```\r\nThe mean out-of-sample accuracy for the validation sample is 0.9967 with a 95% confidence interval of 0.9943 to 0.9982.\r\n\r\nFinal Predictions\r\n========================================================\r\nBecause these out-of-sample accuracy estimates are very good, the final test is to predict the class of 20 unclassified activities.\r\n\r\n```{r,echo=FALSE} \r\nfinalPred <- predict (modelFit, newdata = finaltesting)\r\nfinalPred <- as.character(finalPred)\r\n```\r\n\r\nThe final predictions had the following classes: `r finalPred`. And these were 100% accurate.  \r\n\r\nHowever, taking the upper and lower estimates for the model's out-of-sample accuracy, its accuracy will probably fluxuate between 99.28% and 99.82% at a 95% confidence level.\r\n\r\n\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}